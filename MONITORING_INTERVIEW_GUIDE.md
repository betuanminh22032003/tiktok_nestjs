# ğŸ¯ Monitoring Stack - NgÃ´n Ngá»¯ Phá»ng Váº¥n

## I. Kiáº¿n TrÃºc Tá»•ng QuÃ¡t

### 1. **Logging Pipeline** (Application â†’ Visualization)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NestJS Applications                                          â”‚
â”‚ â”œâ”€ @app/common/logging/logger.service.ts                  â”‚
â”‚ â”‚  â””â”€ Winston [Console | File | Elasticsearch Transport] â”‚
â”‚ â””â”€ HTTP Interceptor + Middleware (Automatic)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                     â†“                     â†“
   [Logstash]          [Loki Agent]          [Sentry SDK]
   (Processing)        (Shipping)            (Errors)
        â†“                     â†“                     â†“
   [Elasticsearch]      [Loki DB]             [Sentry]
   (Storage)            (Storage)             (Storage)
        â†“                     â†“                     â†“
   [Kibana]             [Grafana]             [Sentry UI]
   (Analytics)          (Dashboard)           (Alerting)
```

### 2. **Metrics Pipeline** (Prometheus)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NestJS Services                                              â”‚
â”‚ â”œâ”€ prom-client library (Counters, Histograms, Gauges)      â”‚
â”‚ â””â”€ /metrics endpoint on each service                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    [Prometheus Scraper]
                    (Pulls every 15s)
                              â†“
                    [Time-Series Database]
                    (30-day retention)
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                   â†“                   â†“
   [Recording Rules]  [Alert Rules]      [Grafana Queries]
   (Pre-computed)     (Detect issues)    (Dashboard)
```

---

## II. CÃ¡c ThÃ nh Pháº§n ChÃ­nh & So SÃ¡nh

### A. **Logging Tools**

| ThÃ nh Pháº§n        | Role           | Output             | Má»¥c ÄÃ­ch                       |
| ----------------- | -------------- | ------------------ | ------------------------------ |
| **Winston**       | App Logger     | Structured JSON    | Capture chi tiáº¿t má»i sá»± kiá»‡n   |
| **Logstash**      | Log Processor  | Parsed Events      | Filter, enrich, transform logs |
| **Elasticsearch** | Log Storage    | Indexed Documents  | Full-text search, analytics    |
| **Kibana**        | Log UI         | Search Interface   | TÃ¬m root cause tá»« logs         |
| **Loki**          | Log Aggregator | Label-indexed Logs | Lightweight, Grafana-native    |
| **Promtail**      | Log Shipper    | Forwarded Logs     | Gá»­i logs Ä‘áº¿n Loki              |

**Khi nÃ o dÃ¹ng gÃ¬?**

- **Chá»‰ cáº§n logging Ä‘Æ¡n giáº£n**: Winston + File
- **Cáº§n search logs**: Winston + Logstash + Elasticsearch + Kibana
- **Tá»‘i Æ°u chi phÃ­ + Grafana**: Winston + Loki + Promtail

### B. **Monitoring Tools**

| ThÃ nh Pháº§n          | TÃ­nh NÄƒng            | GiÃ¡ Trá»‹                        |
| ------------------- | -------------------- | ------------------------------ |
| **Prometheus**      | Metrics TSDB         | Real-time time-series data     |
| **Grafana**         | Visualization        | Unified dashboard              |
| **Recording Rules** | Pre-computed Queries | Faster queries, reduced load   |
| **Alert Rules**     | Alert Conditions     | Detect problems early          |
| **Alertmanager**    | Alert Routing        | Route alerts to right channels |

### C. **Error Tracking & Tracing**

| ThÃ nh Pháº§n        | Capture    | Alert     | Trace               |
| ----------------- | ---------- | --------- | ------------------- |
| **Sentry**        | Exceptions | Immediate | Error source        |
| **Jaeger**        | Requests   | No        | Latency per service |
| **Elasticsearch** | All logs   | Manual    | Trace via log IDs   |

---

## III. CÃ¡ch Hoáº¡t Äá»™ng Chi Tiáº¿t

### **Flow 1: User gáº·p lá»—i**

```
1. User hits API endpoint
   â†“
2. @LoggingInterceptor logs request
   â†“
3. Error occurs (NotFoundException)
   â†“
4. SentryService catches exception
   â†“
5. Sentry sends alert to Slack
   â†“
6. Winston logs: { level: 'error', message: '...', error: {...} }
   â†“
7. Logstash parses the error
   â†“
8. Elasticsearch stores structured log
   â†“
9. Engineer searches Kibana for 'NotFoundException'
   â†“
10. Finds root cause â†’ Fix â†’ Deploy
```

### **Flow 2: Slow queries detected**

```
1. Database query takes 2 seconds (vs 100ms average)
   â†“
2. MetricsService records:
   - http_request_duration_seconds histogram
   - database_query_duration_seconds histogram
   â†“
3. Prometheus scrapes metrics every 15s
   â†“
4. Recording rule evaluates:
   histogram_quantile(0.95, database_query_duration_seconds)
   Result: 1500ms (above threshold)
   â†“
5. Alert rule triggers: database_query_slow
   â†“
6. Alertmanager routes to Slack + PagerDuty
   â†“
7. On-call engineer gets alert
   â†“
8. Checks Grafana dashboard â†’ sees database is overloaded
   â†“
9. Scales database or optimizes query
```

### **Flow 3: Service-to-service request (Jaeger tracing)**

```
Client Request
  â†“ [Jaeger Instrumentation]
â”Œâ”€ API Gateway (span_id: 1)  â”‚ 10ms
â”‚  â””â”€ Authenticate call      â”‚ 30ms
â”‚     â””â”€ Auth Service        â”‚ 20ms
â”‚  â””â”€ Fetch Video call       â”‚ 150ms
â”‚     â””â”€ Video Service       â”‚ 140ms
â”‚        â””â”€ PostgreSQL query â”‚ 80ms
â”œâ”€ Response assembly         â”‚ 5ms
â†“
Total Latency: 195ms

Jaeger shows:
- Each span with timing
- Which service is bottleneck (Video Service: 140ms)
- Network overhead (10ms waiting)
```

---

## IV. Advanced Concepts

### **1. Recording Rules - Táº¡i sao quan trá»ng?**

```promql
// Without Recording Rules (Expensive)
histogram_quantile(
  0.95,
  rate(http_request_duration_seconds_bucket[5m])
)

Cost: Must evaluate 10,000 time series every time query runs
Latency: Dashboard refresh slow (1-5s per panel)


// With Recording Rules (Optimized)
- record: http:request_duration:p95
  interval: 1m
  expr: histogram_quantile(0.95, rate(...))

Then query just: http:request_duration:p95
Cost: Pre-computed every minute
Latency: Fast response (ms)
```

### **2. Alert Fatigue Management**

```yaml
# Problem: Too many alerts (boy who cried wolf)
- alert: ErrorOccurred
  expr: errors > 0 # Fires millions of times daily

# Solution: Smart thresholds + Duration
- alert: HighErrorRate
  expr: rate(errors[5m]) / rate(total[5m]) > 0.05
  for: 5m # Only if sustained for 5 minutes
  annotations:
    summary: 'Error rate {{ $value }}% for 5m'
```

### **3. Log Sampling Strategy**

```
Production vs Development:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DEV                                  â”‚
â”‚ Log Level: DEBUG (very verbose)     â”‚
â”‚ Retention: 1 day (local)            â”‚
â”‚ Elasticsearch: None                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRODUCTION                           â”‚
â”‚ Log Level: INFO (critical events)   â”‚
â”‚ Retention: 30 days (hot), 90 (warm) â”‚
â”‚ Elasticsearch: All INFO+ logs       â”‚
â”‚ Sampling: 1% of trace_ids for Jaeger
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Result: Balanced debugging + cost-efficiency
```

### **4. SLI vs SLO vs Alert Threshold**

```
â”Œâ”€ SLI (Service Level Indicator) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "P95 latency is 200ms"                       â”‚
â”‚ "Error rate is 0.1%"                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ SLO (Service Level Objective) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "P95 latency < 200ms, 99.9% of time"        â”‚
â”‚ "Error rate < 0.1%, 99.9% of time"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Alert Threshold â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ When: P95 latency > 300ms for 5m             â”‚
â”‚ When: Error rate > 1%                        â”‚
â”‚ (More aggressive than SLO to catch issues)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## V. Phá»ng Váº¥n - Common Questions

### **Q1: "Describe logging architecture in your project"**

**A (Structured Answer)**:

```
æˆ‘ä»¬ä½¿ç”¨ ELK Stack (Elasticsearch + Logstash + Kibana) é…åˆ Winston:

1. Application Layer:
   - Winston logger captures structured logs
   - Automatic HTTP interceptor logs requests
   - Different transports: console, file rotation, Elasticsearch

2. Processing Layer:
   - Logstash receives logs via syslog/file input
   - Parses JSON format, extracts fields
   - Enriches with service name, environment

3. Storage Layer:
   - Elasticsearch indexes logs (searchable)
   - Index rotation daily (optimize storage)
   - Time-based index: logs-YYYY.MM.dd

4. Visualization:
   - Kibana provides search interface
   - Create dashboards for specific services
   - Alert when error_count > threshold

Benefits:
- Centralized logging across microservices
- Full-text search for debugging
- Historical analysis for performance tuning
```

### **Q2: "How do you handle high volume logging?"**

**A**:

```
Challenge: 1000 requests/sec = millions of logs/day

Solution:

1. Log Sampling
   - Log 100% of ERRORS always
   - Log 10% of INFO level
   - Skip DEBUG in production

2. Async Logging
   - Winston uses buffer (don't block requests)
   - Batch sends to Elasticsearch

3. Log Rotation
   - Daily index rotation
   - Archive old indexes to S3

4. Filtering
   - Health check logs (exclude from Elasticsearch)
   - Keep only relevant fields
```

### **Q3: "Sentry vs Elasticsearch for error tracking"**

**A**:

```
Elasticsearch:
âœ… All logs + searchable
âœ… Historical analysis
âŒ Manual alert setup
âŒ Noise from INFO/DEBUG

Sentry:
âœ… Real-time error alerts
âœ… Source maps, stack traces
âœ… Automatic alert grouping
âŒ Expensive for high volume
âŒ Limited log searching

Best Practice:
- Use BOTH
- Sentry: Real-time + alerts
- Elasticsearch: Historical + analysis
```

### **Q4: "Recording rules - why needed?"**

**A**:

```
Without Recording Rules:
- Dashboard with 10 panels
- Each panel runs complex query
- Prometheus evaluates 50,000 time series per query
- Refresh takes 10 seconds âŒ

With Recording Rules:
- Pre-compute expensive queries every minute
- Store results as new time series
- Dashboard just queries pre-computed data
- Refresh takes 100ms âœ…

Trade-off:
- Storage: +20% more disk (worth it)
- CPU: Spreads evaluation evenly
- Query latency: 100x faster
```

### **Q5: "How do you ensure alerts don't go to wrong team?"**

**A**:

```yaml
Alertmanager Routing:

route:
  receiver: 'default'
  routes:
    - match:
        service: auth
        severity: critical
      receiver: auth-team-pagerduty # Immediate
      group_wait: 10s

    - match:
        service: video
        severity: warning
      receiver: video-team-slack # Grouped
      group_wait: 30s

default_receiver: ops-team-slack

Result:
  - Auth team: Critical errors â†’ PagerDuty SMS
  - Video team: Warnings â†’ Slack (grouped, less noise)
  - Ops: Everything else â†’ Default Slack
```

---

## VI. Practical Implementation Examples

### **Example 1: Logging a Database Error**

```typescript
@Injectable()
export class UserRepository {
  constructor(private logger: CustomLoggerService) {
    this.logger.setContext('UserRepository');
  }

  async findById(id: string) {
    try {
      return await this.db.query(`SELECT * FROM users WHERE id = $1`, [id]);
    } catch (error) {
      // Structured error log
      this.logger.error('Failed to find user', {
        userId: id,
        error: error.message,
        stack: error.stack,
        code: error.code,
      });
      // Automatically captured by Sentry too
      throw new InternalServerErrorException('Database error');
    }
  }
}

// In Elasticsearch:
{
  "timestamp": "2025-01-15T10:30:45Z",
  "level": "ERROR",
  "context": "UserRepository",
  "message": "Failed to find user",
  "userId": "123",
  "error": "column 'id' not found",
  "code": "42703"
}

// Query in Kibana:
"Failed to find user" AND code: 42703
-> Find all instances of this error
```

### **Example 2: Recording Custom Metrics**

```typescript
@Controller('/api/payments')
export class PaymentController {
  constructor(private metrics: MetricsService) {}

  @Post()
  async createPayment(@Body() dto: CreatePaymentDto) {
    const start = Date.now();

    try {
      const result = await this.paymentService.charge(dto);

      // Record success
      this.metrics.recordHttpRequest('POST', '/api/payments', 200, Date.now() - start);
      this.metrics.recordDatabaseQuery('INSERT', 'payments', Date.now() - start);

      return result;
    } catch (error) {
      // Record failure
      this.metrics.recordHttpRequest('POST', '/api/payments', 500, Date.now() - start);

      throw error;
    }
  }
}

// In Prometheus:
# HELP http_requests_total Total HTTP requests
# TYPE http_requests_total counter
http_requests_total{method="POST",path="/api/payments",status="200"} 150
http_requests_total{method="POST",path="/api/payments",status="500"} 3

# HELP http_request_duration_seconds Request latency
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{path="/api/payments",le="0.1"} 100
http_request_duration_seconds_bucket{path="/api/payments",le="0.5"} 148
http_request_duration_seconds_bucket{path="/api/payments",le="1.0"} 150
```

---

## VII. Quick Reference - Interview Checklist

- [ ] Explain logging pipeline (Winston â†’ Logstash â†’ ES â†’ Kibana)
- [ ] Explain metrics pipeline (prom-client â†’ Prometheus â†’ Grafana)
- [ ] Why Recording Rules? (Pre-computation for speed)
- [ ] Alert routing strategy (Critical vs Warning vs Info)
- [ ] Sentry vs Elasticsearch (Real-time vs Historical)
- [ ] Jaeger purpose (Distributed tracing for latency)
- [ ] Log sampling (100% errors, 10% info)
- [ ] SLI vs SLO (What vs Goal)
- [ ] Handle high volume (Sampling, async, rotation)
- [ ] Common monitoring mistakes (Too many alerts, no sampling, expensive queries)

---

**Created**: December 7, 2025
**Version**: 1.0 - Production Ready
