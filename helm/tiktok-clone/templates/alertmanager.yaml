{{- if .Values.monitoring.enabled }}
{{- if .Values.monitoring.prometheus.enabled }}
---
# AlertManager ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: {{ .Values.namespace }}
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m

    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default'
      routes:
        - match:
            severity: critical
          receiver: 'critical'
          continue: true
        - match:
            severity: warning
          receiver: 'warning'

    receivers:
      - name: 'default'
        webhook_configs:
          - url: 'http://localhost:5001/webhook'
            send_resolved: true

      - name: 'critical'
        webhook_configs:
          - url: 'http://localhost:5001/webhook/critical'
            send_resolved: true

      - name: 'warning'
        webhook_configs:
          - url: 'http://localhost:5001/webhook/warning'
            send_resolved: true

    inhibit_rules:
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'cluster', 'service']

---
# Alert Rules ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alert-rules
  namespace: {{ .Values.namespace }}
data:
  alert_rules.yml: |
    groups:
      - name: tiktok_clone_alerts
        interval: 30s
        rules:
          # Service Down Alerts
          - alert: ServiceDown
            expr: up == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Service {{`{{`}} $labels.job {{`}}`}} is down"
              description: "{{`{{`}} $labels.job {{`}}`}} has been down for more than 1 minute."

          # High CPU Usage
          - alert: HighCPUUsage
            expr: rate(process_cpu_seconds_total[5m]) > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage on {{`{{`}} $labels.job {{`}}`}}"
              description: "CPU usage is above 80% for 5 minutes."

          # High Memory Usage
          - alert: HighMemoryUsage
            expr: process_resident_memory_bytes / 1024 / 1024 > 500
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage on {{`{{`}} $labels.job {{`}}`}}"
              description: "Memory usage is above 500MB."

          # High Error Rate
          - alert: HighErrorRate
            expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "High error rate on {{`{{`}} $labels.job {{`}}`}}"
              description: "Error rate is above 5% for 5 minutes."

          # Slow Response Time
          - alert: SlowResponseTime
            expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Slow response time on {{`{{`}} $labels.job {{`}}`}}"
              description: "95th percentile response time is above 1 second."

          # Database Connection Issues
          - alert: DatabaseConnectionPool
            expr: pg_stat_database_numbackends > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High database connections"
              description: "PostgreSQL has more than 80 active connections."

          # Redis Memory High
          - alert: RedisMemoryHigh
            expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Redis memory usage high"
              description: "Redis memory usage is above 90%."

          # Kafka Lag
          - alert: KafkaConsumerLag
            expr: kafka_consumer_lag > 1000
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Kafka consumer lag high"
              description: "Consumer lag is above 1000 messages."

          # Pod Restart
          - alert: PodRestartingTooOften
            expr: rate(kube_pod_container_status_restarts_total[1h]) > 5
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Pod restarting too often"
              description: "Pod {{`{{`}} $labels.pod {{`}}`}} in namespace {{`{{`}} $labels.namespace {{`}}`}} is restarting frequently."

---
# Recording Rules ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-recording-rules
  namespace: {{ .Values.namespace }}
data:
  recording_rules.yml: |
    groups:
      - name: tiktok_clone_recording_rules
        interval: 30s
        rules:
          # Request rate
          - record: job:http_requests:rate5m
            expr: rate(http_requests_total[5m])

          # Error rate
          - record: job:http_errors:rate5m
            expr: rate(http_requests_total{status=~"5.."}[5m])

          # Response time percentiles
          - record: job:http_request_duration:p95
            expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

          - record: job:http_request_duration:p99
            expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))

---
# AlertManager Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: {{ .Values.namespace }}
  labels:
    app: alertmanager
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
        - name: alertmanager
          image: prom/alertmanager:latest
          args:
            - '--config.file=/etc/alertmanager/alertmanager.yml'
            - '--storage.path=/alertmanager'
            - '--web.external-url=http://alertmanager:9093'
          ports:
            - containerPort: 9093
              name: http
          volumeMounts:
            - name: config
              mountPath: /etc/alertmanager
            - name: storage
              mountPath: /alertmanager
          resources:
            requests:
              memory: '128Mi'
              cpu: '100m'
            limits:
              memory: '256Mi'
              cpu: '200m'
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9093
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9093
            initialDelaySeconds: 5
            periodSeconds: 5
      volumes:
        - name: config
          configMap:
            name: alertmanager-config
        - name: storage
          emptyDir: {}

---
# AlertManager Service
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: {{ .Values.namespace }}
  labels:
    app: alertmanager
spec:
  type: ClusterIP
  ports:
    - port: 9093
      targetPort: 9093
      name: http
  selector:
    app: alertmanager

{{- end }}
{{- end }}
